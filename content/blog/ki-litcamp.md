+++
author = "Larissa"
categories = ["Thoughts"]
tags = ["fall2018", "artificial-intelligence", "guest-article", "ai-seminar", "german-only", "litcamp"]
date = "2018-09-09"
description = "guest article - Künstliche Intelligenz auf dem #LitCamp18"
featured = "DSC_1395.JPG"
featuredpath = "date"
linktitle = ""
title = "Algorithmen, die von Katzen träumen"
type = "post"
+++

## guest article from [luanasschreibstube.wordpress.com](https://luanasschreibstube.wordpress.com)

Als ich vor dem [Literaturcamp ](http://literaturcamp-heidelberg.de/)(relativ spontan) auf die Idee kam, eine Diskussionssession über Künstliche Intelligenz anzubieten, hätte ich niemals gedacht, dass sich tatsächlich so viele Leute für das Thema interessieren würden. Letztendlich war der Raum voll, viele Leute hörten zu, was wegen der schlechten Akustik wohl nicht so einfach war, wie ich gehört habe.

Deshalb, und auch weil mich einige von euch darum gebeten hatten, möchte ich das Thema nun noch einmal in einem Blogbeitrag aufgreifen. Dabei werde ich das verschriftlichen, was ich mir vorher als Stütze notiert hatte, und versuchen das mit einzuflechten, was von euch als Diskussionsinput kam. Allerdings ging alles so schnell – ihr müsst entschuldigen, dass ich aus dem Kopf nicht mehr alles rekonstruieren kann, was gesagt wurde.

Für alle, die mich noch nicht kennen: Ich studiere seit drei Semestern [Data Science in Mannheim](https://www.wim.uni-mannheim.de/de/fakultaet/studiengaenge/msc-in-data-science/). Data Science ist ein neuer Studiengang, der in der Schnittmenge zwischen Statistik, Informatik und den Sozialwissenschaften angesiedelt ist. Vereinfacht gesprochen: Wir lernen mit großen Datenmengen umzugehen und dem Computer (einem Programm / einem Algorithmus) beizubringen, wie er sich aus diesen Daten sinnvolles erschließen kann. Diesen Vorgang nennen wir &quot;machine learning&quot;, also [Maschinelles Lernen](https://de.wikipedia.org/wiki/Maschinelles_Lernen), und da Neuronale Netze sowie Künstliche Intelligenz dazu zählen, komme ich im Studium um den Begriff nicht herum. Auf der anderen Seite interessiere ich mich für Literatur und speziell für Science Fiction und lese gerne die alten Klassiker wie [Asimov](https://luanasschreibstube.wordpress.com/tag/asimov/)und [Lem](https://luanasschreibstube.wordpress.com/tag/stanislaw-lem/) oder neue (dystopische) Romane.

Was mich eigentlich zu dieser Session motiviert hat, war eine Beobachtung: In der Stadt sah ich ein Plakat von einem Smartphone-Hersteller, das mit den folgenden Worten (so oder so ähnlich) warb: &quot;Mach jetzt bessere Fotos – mit der neuen Foto-KI im neuen Modell!&quot; Und je aufmerksamer ich dadurch wurde, desto häufiger stieß ich auf den Begriff Künstliche Intelligenz: als Gegner bei Videospielen, als Navigator, Sprachassistent, Fitnesscoach etc. In nahezu jeder App scheint sich so etwas zu verbergen... Aber sind diese Apps und Spiele deshalb wirklich &quot;intelligent&quot;? Was ist Intelligenz überhaupt?

# Anfänge Künstlicher Intelligenz

Der Begriff &quot;Künstliche Intelligenz&quot; (KI) kam das erste Mal im Jahr 1956 auf und beschrieb damals Computer, die mehr tun konnten als &quot;nur&quot; rechnen ([Quelle](http://www.bpb.de/apuz/263678/was-ist-kuenstliche-intelligenz-was-kann-sie-leisten?p=1)). Damals ging es beispielsweise um einen Computer, der Dame spielen konnte. Seit damals gab es Computer, die Schach (1997), Jeopardy (2011) und Go (2017) beherrschten, allesamt besser als der beste Mensch. Allerdings konnten sie eben auch nur das eine: dieses Spiel spielen. (Okay, [Watson](https://de.wikipedia.org/wiki/Watson_(K%C3%BCnstliche_Intelligenz) kann inzwischen auch schon mehr, aber auch nur weil Jeopardy auf dem Prinzip der Sprache beruht und darin ist Watson eben ziemlich gut, aber noch nicht supergut). Setzte man den Computern ein anderes Brettspiel vor, konnten sie Erkenntnisse von dem einen nicht auf das andere übertragen, sondern begannen von Grund auf mit dem &quot;neu&quot; erlernen.

Um Intelligenz und generell menschenähnliches Verhalten besser testen zu können, gilt der von Adam Turin (1950) etablierte [Turing-Test](https://de.wikipedia.org/wiki/Turing-Test) heute als Messlatte. Ein Mensch &quot;chattet&quot; mit einem Computer (in weiteren Stufen kann das auch ein akustisches Gespräch sein). Sobald der Mensch nicht mehr unterscheiden kann (oder es noch nicht einmal bemerkt), ob er mit einem Computer / einer KI oder einem realen Menschen kommuniziert, hat die KI den Turing-Test bestanden. Bisher sei das nur (begrenzt) mit Simulationen von Menschen geglückt, die irgendwelchen Konditionen unterlagen, zum Beispiel eine Assistentin, die einen Friseurtermin vereinbart ([Quelle](https://katzlberger.ai/2018/05/10/hat-die-google-duplex-ki-gerade-den-turing-test-bestanden/)), eine KI, die sich als 13-Jähriger Junge ausgab ([Quelle](https://www.welt.de/debatte/kolumnen/der-onliner/article129391089/Eugene-hat-Turing-Test-nicht-wirklich-bestanden.html)), oder eine, die eine stark ausgeprägte Persönlichkeitsstörung darstellen sollte und mit einem Arzt sprach, der sie für einen Patienten hielt. Doch auch diese KIs sind auf ihre Art und Weise auf eine konkrete Problemstellung begrenzt. Würde man sie in eine andere Situation bringen, sie wüssten nicht, wie sie sich verhalten sollten, wahrscheinlich wären sie noch nicht einmal zu &quot;Verhalten&quot; an sich fähig.

Manche Definitionen machen es sich einfach mit der Beschreibung von Künstlicher Intelligenz: &quot;Ein System gilt als intelligent, wenn es ein Verhalten zeigt, das vom Programmierer ursprünglich nicht vorgesehen wurde.&quot; (Yvonne Hoffstetter). Bei so einem Satz lacht jeder Programmierer kurz auf. Marco [@marcomanders\_da](https://twitter.com/marcomanders_da) beschrieb daraufhin in der Session eine Prüfungssituation: Er sollte ein Programm entwickeln, das in seinen Entscheidungen keinem erkennbaren Muster folgte (dann wäre es intelligent). Es reichte daher vollkommen aus, die Entscheidungen komplett zufällig zu treffen, somit war das Kriterium von Intelligenz schon erfüllt.

# Starke und schwache KIs

Aber reicht das wirklich? Eine umfassendere und differenzierende Definition ist die der starken und schwachen KIs. Eine schwache KI ist in einem bestimmten abgegrenzten Bereich besser (schneller?) als ein Mensch, zum Beispiel im Kalkulieren von Risiken, im Vorhersagen von Verkaufszahlen oder im Vorschlagen von Einkaufsartikeln. In diesem Bereich ist sie einem Menschen durchaus hilfreich. Eine starke KI dagegen verhält sich &quot;wie ein Mensch&quot;, ist also in allen Bereichen der Intelligenz dem Mensch gleichgestellt oder überlegen. Diese Bereiche der Intelligenz umfassen (laut Forschung) emotionale, soziale, räumliche, mathematische, logische und sprachliche Intelligenz. In manchen Bereichen sind manche Programme schon ziemlich gut, in anderen eher weniger. Am Beispiel bei der emotionalen Intelligenz scheint es einen klaren Trennstrich zwischen Mensch und Computer zu geben: Laut Ulrich Eberl (Autor von &quot;Smarte Maschinen&quot; ([https://zukunft2050.wordpress.com](https://zukunft2050.wordpress.com))) sind Computer zwar in der Lage Mimik und daraus Gefühle lesen ([Quelle](https://www.zeit.de/digital/internet/2016-10/deep-learning-ki-besser-als-menschen)), aber schon bei der Spracherkennung gibt es laut Raúl Rojas große Defizite. Dass KIs irgendwann einmal selbst Gefühle ausbilden könnten, hält er für &quot;absolut undenkbar&quot; ([Quelle](http://www.bpb.de/apuz/263675/die-koennen-was-aber-koennen-roboter-auch-fuehlen-)).

Rojas begründet das unter anderem auch dadurch, dass ein Bewusstsein ohne zugehörigen Körper nicht komplett wäre. Auch andere Forscher teilen diese Ansicht: Gefühle und soziale Bindungen entstünden auch und vor allem durch körperlichen Kontakt und andere Sinneswahrnehmungen. Ein Bewusstsein, dem das alles fehlt, kann niemals &quot;menschengleich&quot; werden.

In der Session wurde daraufhin diskutiert, ob es nicht auch ausreicht, wenn ein System eine Emotion glaubhaft &quot;simulieren&quot; würde. Wie findet man heraus, ob ein Gefühl &quot;echt&quot; ist, oder nur &quot;vorgetäuscht&quot;? Muss man wirklich Hormone ausschütten, um wirkliche Angst zu verspüren? Oder reicht auch ein mathematisch berechneter Selbsterhaltungstrieb? Gibt es dazwischen überhaupt einen Unterschied?

Im Moment sind wir also noch weit davon entfernt, ein System zu entwickeln, das tatsächlich &quot;intelligent&quot; im umfassenden Sinne ist. Denn es gibt einige Hürden, die noch zu nehmen sind. Ich möchte zwei kurz aufgreifen:

## Verständnis von Kausalität bzw. Gesunder Menschenverstand

In meiner ersten Statistikvorlesung gab es diese lustige Geschichte von einer statistischen Analyse, die ergab, dass Störche für neugeborene Babys verantwortlich sind. In verschiedenen Landstrichen wurden Störche gezählt und Geburten registriert und siehe da: In Gegenden mit mehr Störchen wurden auch mehr Babys geboren. Natürlich weiß ein Mensch, der vielleicht kurz darüber nachdenkt, dass Störche keinesfalls für mehr Babys sorgen können (dass hier also kein kausaler Zusammenhang besteht). Vielmehr geht es um den Lebensraum, in ländlichen Gebieten finden Störche mehr Futter und die Geburtenrate bei den Menschen ist dort tendenziell höher. Korrelation ist nicht gleich Kausalität! Das allerdings können Algorithmen noch nicht unterscheiden. Sie können zwar ziemlich gute Vorhersagen machen, wie/wann/wo etwas passieren wird, aber nicht sagen WARUM, zumindest nicht in unserem Verständnis. Ja, es gibt Faktoren, die bestimmte Ereignisse wahrscheinlicher machen. Aber sind sie auch der Grund dafür? Um diese Frage zu beantworten braucht es noch immer umfangreiche Experimente, Versuchsanordnungen und Menschen, die darüber nachdenken, ob es logisch ist, ob es Sinn macht, diese und jene Faktoren zu testen.

Zum &quot;Gesunden Menschenverstand&quot; gehört auch, flexibel auf ungewohnte Situationen zu reagieren. Ihr erinnert euch sicher alle an den Fall des selbstfahrenden Tesla-Autos, das mit einen LKW nicht erkannte und dadurch einen tödlichen Unfall verursachte. Das System erkannte den LKW nicht, weil dieser weiß war und somit nicht von den Wolken am Himmel unterscheidbar war ([Quelle](http://www.spiegel.de/auto/aktuell/tesla-die-wichtigsten-fakten-zum-toedlichen-unfall-in-den-usa-a-1100803.html)).  Ein Mensch ist in der Lage, diese Situation (richtig) einzuschätzen, auch wenn er vorher noch nie in seinem Leben einen weißen LKW gesehen hat. Natürlich machen auch Menschen Fehler, viele und ständig, vor allem wenn sie sich in neuen, ungewohnten Situationen befinden. Aber unser Verstand, unsere (Lebens-)Erfahrung helfen uns dabei, gute Entscheidungen zu treffen.

Vielleicht liegt das auch daran, dass heutige Systeme eben &quot;nur&quot; einseitig trainiert werden. Sie sollen Katzen erkennen? Dann bekommen sie eben Bilder von Katzen. Und erkennen plötzlich in Fotos von Wolken nur das eine: Katzen ([Quelle](http://www.bpb.de/apuz/263678/was-ist-kuenstliche-intelligenz-was-kann-sie-leisten?p=2)) (siehe auch hier: [coole Quelle](https://www.businessinsider.com/these-trippy-images-show-how-googles-ai-sees-the-world-2015-6?IR=T#ai-trained-to-identify-places-and-building-features-spat-out-the-trippiest-images-like-these-blue-and-green-arches-8)). Einen ähnlichen Fall gab es bei dem Erkennen von Panzern auf Bildern, bei dem das System aber eher das Wetter erkannte, weil Panzer immer nur unter blauem Himmel abgelichtet wurden. Nun ist es aber so, dass wir das Programm nicht fragen können, WARUM es jenes Foto als Katze / Panzer einordnet, und ein anderes nicht, die Bauweise eines Neuronalen Netzes lässt das einfach nicht zu (oder es ist reine Statistik: [Quelle](http://www.spiegel.de/spiegel/kuenstliche-intelligenz-selbstlernende-software-laesst-sich-leicht-taeuschen-a-1191991.html)). Anders als bei uns Menschen. Wie würde sich wohl ein System entwickeln, das wie ein Mensch aufwächst, mit allen Erfahrungen und Eindrücken? Würde es diesen &quot;Verstand&quot; entwickeln, würde es einen Sinn für das Geschehen in der Welt bekommen?

## Reproduktion von Vorurteilen

Da Algorithmen eben nur aus dem lernen können, was wir ihnen vorsetzen und was wir selbst auch vorleben, tendieren solche Programme (wie zum Beispiel Vorhersage-Algorithmen) dazu, gesellschaftlich verinnerlichte Vorurteile zu reproduzieren. Ich möchte zu diesem Thema immer wieder das Buch &quot;Weapons of Math Destruction&quot; von Cathy O&#39;Neil empfehlen (auf Deutsch &quot;Angriff der Algorithmen&quot; [auf Goodreads](https://www.goodreads.com/book/show/36099033-angriff-der-algorithmen?ac=1&amp;from_search=true)), das diese Probleme auf einem nicht-technischen Niveau erklärt und einem die Augen öffnet, vor allem wenn man als Sozialwissenschaftler\_in o.Ä. selbst mit Datenerhebung und -analyse beschäftigt ist. Die Frage ist deshalb noch offen, ob wir es schaffen Algorithmen und Künstlicher Intelligenz etwas zu verinnerlichen (einzuprogrammieren), zu dem wir selbst nicht in der Lage sind: Neutralität allen gegenüber, Wertempfinden für biologisches Leben, mit den Paradoxen menschlicher Moral und Ethik umzugehen. Oder enden doch alle Experimente wie der Chatbot von Microsoft ([Quelle](https://www.sueddeutsche.de/digital/microsoft-programm-tay-rassistischer-chat-roboter-mit-falschen-werten-bombardiert-1.2928421#redirectedFromLandingpage)), der von den falschen Leuten im Internet lernte und zu einem Rassisten wurde. Die &quot;Metaregeln&quot;, die zukünftig verhindern sollen, dass so etwas passiert… Sind das die Hürden, die der Entwicklung einer KI im Wege stehen? Und wenn es irgendwann eine KI gibt, können wir diese Metaregeln überhaupt noch bestimmen, oder entscheidet die KI irgendwann für uns, was ethisch korrektes Verhalten ist?

Und ist das der Grund, weshalb allmächtige KIs in Literatur und Filmen als negativ, als Bedrohung dargestellt werden? (Nicht immer, wurde in der Session gesagt, aber zumindest ich spüre da eine Tendenz.) Weil eine KI, die alles besser kann als der Mensch, dem Menschen seine Freiheit nimmt, weil dieser per Definition fehlerhaft ist? Weil wir dann alle in einem goldenen Käfig enden, in dem uns nichts passieren kann?

So viele offene Fragen. So viele spannende Bücher. So wenig Zeit, um darüber nachzudenken. Ich schreibe diesen Beitrag nicht aus Zufall zu diesem Augenblick. In diesem Semester werden wir uns ein ganzes Seminar lang mit all den Fragen rund um Künstliche Intelligenz beschäftigen. Seid also gespannt, schickt mir Fragen und Büchertipps und erwartet vielleicht auf den Literaturcamp 2019 eine spannende Fortsetzung dieser Session :)

# Weiterführende Informationen

Bundeszentrale für politische Bildung - Aus Politik und Zeitgeschehen: Künstliche Intelligenz [http://www.bpb.de/apuz/263673/kuenstliche-intelligenz](http://www.bpb.de/apuz/263673/kuenstliche-intelligenz)

Auch ganz gut: Übersichtsseite von Spiegel Online: [http://www.spiegel.de/netzwelt/gadgets/kuenstliche-intelligenz-turing-test-chatbots-neuronale-netzwerke-a-1126718.html](http://www.spiegel.de/netzwelt/gadgets/kuenstliche-intelligenz-turing-test-chatbots-neuronale-netzwerke-a-1126718.html)

Gut als Einstieg und für solche, die noch mehr wissen wollen: Nick Bostrom - Superintelligenz [https://www.goodreads.com/book/show/25395098-superintelligenz](https://www.goodreads.com/book/show/25395098-superintelligenz)

Für KI in Büchern könnt ihr gerne meine Goodreads-Liste durchstöbern, da habe ich alles gesammelt, was mir aus allen möglichen Ecken (von Twitter, aus der Uni, von Freunden und Bekannten) empfohlen wurde, natürlich ist diese Liste lange nicht vollständig, also wenn jemand ein Buch darauf vermisst, immer her damit :) [https://www.goodreads.com/review/list/43500674-luana-casado?shelf=ai-thesis](https://www.goodreads.com/review/list/43500674-luana-casado?shelf=ai-thesis)

